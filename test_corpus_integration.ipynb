{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Corpus Integration (Notebook)\n",
        "\n",
        "This notebook mirrors the checks from `test_corpus_integration.py` and runs them interactively:\n",
        "\n",
        "- Verify `corpus.py` uses `tokenizer.py`\n",
        "- Tokenization consistency check\n",
        "- Build/load inverted index from `corpus.csv` and search with test text\n",
        "- Search engine tests on `corpus.csv`\n",
        "- Build unigrams from `corpus.csv`\n",
        "\n",
        "Requires files in project root:\n",
        "- `corpus.csv`\n",
        "- (Optional) `inverted_index.json`, `test_inverted_index.json` (will be built if missing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "# !pip install pandas\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from newa_nlp import (\n",
        "    tokenize_text,\n",
        "    build_unigram,\n",
        "    build_unigram_from_csv,\n",
        "    InvertedIndex,\n",
        "    build_inverted_index_from_csv,\n",
        "    save_inverted_index,\n",
        "    load_inverted_index,\n",
        "    create_search_engine,\n",
        ")\n",
        "\n",
        "TEST_TEXT = \"नेपाल भाषा नेवाः भाषा खः।\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Direct tokens: ['नेपाल', 'भाषा', 'नेवाः', 'भाषा', 'खः']\n",
            "Unigram tokens: ['भाषा', 'खः', 'नेपाल', 'नेवाः']\n",
            "Consistent: True\n"
          ]
        }
      ],
      "source": [
        "# 1) Corpus-tokenizer integration and tokenization consistency\n",
        "\n",
        "# Direct tokenization\n",
        "tokens_direct = tokenize_text(TEST_TEXT, mode=\"regex\")\n",
        "print(\"Direct tokens:\", tokens_direct)\n",
        "\n",
        "# Unigram-based tokenization\n",
        "unigrams = build_unigram([TEST_TEXT], tokenizer_mode=\"regex\")\n",
        "print(\"Unigram tokens:\", [t for t, _ in unigrams])\n",
        "\n",
        "print(\"Consistent:\", set(tokens_direct) == set(t for t, _ in unigrams))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing index from test_inverted_index.json ...\n",
            "Inverted index loaded from: test_inverted_index.json\n",
            "Test tokens: ['नेपाल', 'भाषा', 'नेवाः', 'भाषा', 'खः']\n",
            "'नेपाल': 9364 docs, sample: ['wiki_13391.txt', 'wiki_47988.txt', 'wiki_13254.txt']\n",
            "'भाषा': 31610 docs, sample: ['wiki_56380.txt', 'wiki_60684.txt', 'wiki_56674.txt']\n",
            "'नेवाः': 1389 docs, sample: ['nepalmandal_2424.txt', 'nepalmandal_746.txt', 'nepalmandal_5232.txt']\n",
            "'भाषा': 31610 docs, sample: ['wiki_56380.txt', 'wiki_60684.txt', 'wiki_56674.txt']\n",
            "'खः': 49201 docs, sample: ['wiki_26281.txt', 'wiki_25178.txt', 'wiki_43590.txt']\n",
            "AND 'नेपाल' & 'भाषा': 7277 docs\n",
            "OR  'नेपाल' | 'भाषा': 33697 docs\n"
          ]
        }
      ],
      "source": [
        "# 2) Build or load inverted index from corpus.csv and search with test text\n",
        "\n",
        "if not os.path.exists('corpus.csv'):\n",
        "    raise FileNotFoundError(\"corpus.csv not found in current directory\")\n",
        "\n",
        "index_file = 'test_inverted_index.json'\n",
        "\n",
        "if os.path.exists(index_file):\n",
        "    print(f\"Loading existing index from {index_file} ...\")\n",
        "    index = load_inverted_index(index_file)\n",
        "else:\n",
        "    print(\"Building new inverted index from corpus.csv ...\")\n",
        "    index = build_inverted_index_from_csv(\n",
        "        csv_path='corpus.csv',\n",
        "        doc_id_column='filename',\n",
        "        content_column='content',\n",
        "        tokenizer_mode='regex',\n",
        "    )\n",
        "    print(\"Saving index ...\")\n",
        "    save_inverted_index(index, index_file, format='json')\n",
        "\n",
        "# Search with tokens from TEST_TEXT\n",
        "test_tokens = tokenize_text(TEST_TEXT, mode='regex')\n",
        "print(\"Test tokens:\", test_tokens)\n",
        "\n",
        "for term in test_tokens:\n",
        "    docs = index.search([term])\n",
        "    print(f\"'{term}': {len(docs)} docs, sample: {list(docs)[:3]}\")\n",
        "\n",
        "if len(test_tokens) >= 2:\n",
        "    and_docs = index.search(test_tokens[:2], operation='AND')\n",
        "    or_docs = index.search(test_tokens[:2], operation='OR')\n",
        "    print(f\"AND '{test_tokens[0]}' & '{test_tokens[1]}': {len(and_docs)} docs\")\n",
        "    print(f\"OR  '{test_tokens[0]}' | '{test_tokens[1]}': {len(or_docs)} docs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building main inverted index (once) ...\n",
            "[1000/80379] Processed 1000 documents\n",
            "[2000/80379] Processed 2000 documents\n",
            "[3000/80379] Processed 3000 documents\n",
            "[4000/80379] Processed 4000 documents\n",
            "[5000/80379] Processed 5000 documents\n",
            "[6000/80379] Processed 6000 documents\n",
            "[7000/80379] Processed 7000 documents\n",
            "[8000/80379] Processed 8000 documents\n",
            "[9000/80379] Processed 9000 documents\n",
            "[10000/80379] Processed 10000 documents\n",
            "[11000/80379] Processed 11000 documents\n",
            "[12000/80379] Processed 12000 documents\n",
            "[13000/80379] Processed 13000 documents\n",
            "[14000/80379] Processed 14000 documents\n",
            "[15000/80379] Processed 15000 documents\n",
            "[16000/80379] Processed 16000 documents\n",
            "[17000/80379] Processed 17000 documents\n",
            "[18000/80379] Processed 18000 documents\n",
            "[19000/80379] Processed 19000 documents\n",
            "[20000/80379] Processed 20000 documents\n",
            "[21000/80379] Processed 21000 documents\n",
            "[22000/80379] Processed 22000 documents\n",
            "[23000/80379] Processed 23000 documents\n",
            "[24000/80379] Processed 24000 documents\n",
            "[25000/80379] Processed 25000 documents\n",
            "[26000/80379] Processed 26000 documents\n",
            "[27000/80379] Processed 27000 documents\n",
            "[28000/80379] Processed 28000 documents\n",
            "[29000/80379] Processed 29000 documents\n",
            "[30000/80379] Processed 30000 documents\n",
            "[31000/80379] Processed 31000 documents\n",
            "[32000/80379] Processed 32000 documents\n",
            "[33000/80379] Processed 33000 documents\n",
            "[34000/80379] Processed 34000 documents\n",
            "[35000/80379] Processed 35000 documents\n",
            "[36000/80379] Processed 36000 documents\n",
            "[37000/80379] Processed 37000 documents\n",
            "[38000/80379] Processed 38000 documents\n",
            "[39000/80379] Processed 39000 documents\n",
            "[40000/80379] Processed 40000 documents\n",
            "[41000/80379] Processed 41000 documents\n",
            "[42000/80379] Processed 42000 documents\n",
            "[43000/80379] Processed 43000 documents\n",
            "[44000/80379] Processed 44000 documents\n",
            "[45000/80379] Processed 45000 documents\n",
            "[46000/80379] Processed 46000 documents\n",
            "[47000/80379] Processed 47000 documents\n",
            "[48000/80379] Processed 48000 documents\n",
            "[49000/80379] Processed 49000 documents\n",
            "[50000/80379] Processed 50000 documents\n",
            "[51000/80379] Processed 51000 documents\n",
            "[52000/80379] Processed 52000 documents\n",
            "[53000/80379] Processed 53000 documents\n",
            "[54000/80379] Processed 54000 documents\n",
            "[55000/80379] Processed 55000 documents\n",
            "[56000/80379] Processed 56000 documents\n",
            "[57000/80379] Processed 57000 documents\n",
            "[58000/80379] Processed 58000 documents\n",
            "[59000/80379] Processed 59000 documents\n",
            "[60000/80379] Processed 60000 documents\n",
            "[61000/80379] Processed 61000 documents\n",
            "[62000/80379] Processed 62000 documents\n",
            "[63000/80379] Processed 63000 documents\n",
            "[64000/80379] Processed 64000 documents\n",
            "[65000/80379] Processed 65000 documents\n",
            "[66000/80379] Processed 66000 documents\n",
            "[67000/80379] Processed 67000 documents\n",
            "[68000/80379] Processed 68000 documents\n",
            "[69000/80379] Processed 69000 documents\n",
            "[70000/80379] Processed 70000 documents\n",
            "[71000/80379] Processed 71000 documents\n",
            "[72000/80379] Processed 72000 documents\n",
            "[73000/80379] Processed 73000 documents\n",
            "[74000/80379] Processed 74000 documents\n",
            "[75000/80379] Processed 75000 documents\n",
            "[76000/80379] Processed 76000 documents\n",
            "[77000/80379] Processed 77000 documents\n",
            "[78000/80379] Processed 78000 documents\n",
            "[79000/80379] Processed 79000 documents\n",
            "[80000/80379] Processed 80000 documents\n",
            "[80379/80379] Processed 80379 documents\n",
            "Inverted index built successfully!\n",
            "Documents processed: 80380\n",
            "Unique terms: 253538\n",
            "Total term occurrences: 9378883\n",
            "Inverted index saved to: inverted_index.json\n",
            "Inverted index loaded from: inverted_index.json\n",
            "Query 'नेपाल': 5 docs, sample: ['wiki_13391.txt', 'wiki_47988.txt', 'wiki_13254.txt']\n",
            "Query 'भाषा': 5 docs, sample: ['wiki_56380.txt', 'wiki_60684.txt', 'wiki_56674.txt']\n",
            "Query 'नेवाः': 5 docs, sample: ['nepalmandal_2424.txt', 'nepalmandal_5232.txt', 'nepalmandal_746.txt']\n",
            "Query 'नेपाल भाषा': 5 docs, sample: ['wiki_13391.txt', 'wiki_13254.txt', 'wiki_41719.txt']\n"
          ]
        }
      ],
      "source": [
        "# 3) Search engine tests on corpus.csv\n",
        "\n",
        "se_index_file = 'inverted_index.json'\n",
        "\n",
        "if not os.path.exists(se_index_file):\n",
        "    print(\"Building main inverted index (once) ...\")\n",
        "    se_index = build_inverted_index_from_csv(\n",
        "        csv_path='corpus.csv',\n",
        "        doc_id_column='filename',\n",
        "        content_column='content',\n",
        "        tokenizer_mode='regex',\n",
        "    )\n",
        "    save_inverted_index(se_index, se_index_file, format='json')\n",
        "else:\n",
        "    print(f\"Using existing {se_index_file}\")\n",
        "\n",
        "engine = create_search_engine(se_index_file, 'corpus.csv')\n",
        "\n",
        "queries = ['नेपाल', 'भाषा', 'नेवाः', 'नेपाल भाषा']\n",
        "for q in queries:\n",
        "    docs = engine.search_documents(q, limit=5)\n",
        "    print(f\"Query '{q}': {len(docs)} docs, sample: {docs[:3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building top 20 unigrams (freq) ...\n",
            " 1. थ्व: 455,768\n",
            " 2. दु: 211,478\n",
            " 3. थाय्: 210,236\n",
            " 4. व: 185,876\n",
            " 5. भाषा: 170,387\n",
            " 6. खः: 96,361\n",
            " 7. ख: 81,789\n",
            " 8. या: 79,930\n",
            " 9. थासय्: 75,808\n",
            "10. नं: 72,317\n",
            "11. कथं: 66,987\n",
            "12. छगू: 64,549\n",
            "13. भारतया: 63,949\n",
            "14. खने: 61,233\n",
            "15. संकिपा: 61,068\n",
            "16. निसें: 57,713\n",
            "17. छ्येलिगु: 55,424\n",
            "18. राज्यया: 54,910\n",
            "19. कुल: 54,171\n",
            "20. स्वापू: 52,797\n",
            "\n",
            "Top 10 terms (Devanagari sorted) ...\n",
            " 1. ँचयायाँ: 1\n",
            " 2. ँन्हूगू: 2\n",
            " 3. ँस्वगिनलय्: 1\n",
            " 4. ं: 4\n",
            " 5. ंं: 1\n",
            " 6. ंःएइल्ल्: 1\n",
            " 7. ंआॠआ: 1\n",
            " 8. ंइयगव: 1\n",
            " 9. ंऍ: 1\n",
            "10. ंगुठी: 1\n"
          ]
        }
      ],
      "source": [
        "# 4) Unigram tests on corpus.csv\n",
        "\n",
        "print(\"Building top 20 unigrams (freq) ...\")\n",
        "unigrams = build_unigram_from_csv(\n",
        "    csv_path='corpus.csv',\n",
        "    content_column='content',\n",
        "    tokenizer_mode='regex',\n",
        "    sort_by='freq',\n",
        "    top_k=20,\n",
        ")\n",
        "for i, (term, count) in enumerate(unigrams, 1):\n",
        "    print(f\"{i:2d}. {term}: {count:,}\")\n",
        "\n",
        "print(\"\\nTop 10 terms (Devanagari sorted) ...\")\n",
        "unigrams_dev = build_unigram_from_csv(\n",
        "    csv_path='corpus.csv',\n",
        "    content_column='content',\n",
        "    tokenizer_mode='regex',\n",
        "    sort_by='dev',\n",
        "    top_k=10,\n",
        ")\n",
        "for i, (term, count) in enumerate(unigrams_dev, 1):\n",
        "    print(f\"{i:2d}. {term}: {count:,}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
